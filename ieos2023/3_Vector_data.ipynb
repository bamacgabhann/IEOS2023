{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bamacgabhann/IEOS2023/blob/main/ieos2023/3_Vector_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-2xUUAfX0xa"
   },
   "source": [
    "# Vector Data\n",
    "\n",
    "In the Data Types notebook, we looked at how computers can store data as text strings (str), whole number integers (int), decimal number floats (float), dates, times, and number of other formats. One we mentioned specifically was coordinates, for which I mentioned the idea of coordinates as tuples.\n",
    "\n",
    "Tuples can be used for coordinates because coordinates don't come as single numbers. We'll look at different forms of coordinates soon, but even the simplest coordinates come as at least two numbers - an x coordinate, and a y coordinate. We use tuples so that we can store those numbers together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOZlcuk6cHXE",
    "outputId": "a24be415-4885-413b-d3e4-afc8a771786f"
   },
   "outputs": [],
   "source": [
    "x_coordinate = 5\n",
    "y_coordinate = 9\n",
    "\n",
    "x_coordinate, y_coordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6t5RakCbcZRH"
   },
   "source": [
    "This is not very useful, because we'd have to keep referencing both. Much easier to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUwQPGcVcfv3",
    "outputId": "f80029bc-84ea-4fec-b9a0-dab3719d5276"
   },
   "outputs": [],
   "source": [
    "my_coordinates = (5, 9)\n",
    "my_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITJE-4MMeOg6",
    "outputId": "88f9dadc-3fca-450b-dc50-e4deeb0ded12"
   },
   "outputs": [],
   "source": [
    "type(my_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4K0Sup5iOvT"
   },
   "source": [
    "It just keeps things simpler to store the pairs of coordinates together.\n",
    "\n",
    "You don't need to define your own coordinates, though, because the module Shapely already has classes which define data types for coordinates of shapes. These are the building blocks of geospatial data processing. \n",
    "\n",
    "In the following sections, we'll use these, along with the module matplotlib to draw plots, and GeoPandas to be an intermediary between Shapely and matplotlib. We'll come back to GeoPandas properly in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwOReuKpcll-"
   },
   "source": [
    "## 1. Points\n",
    "\n",
    "This coordinate refers to a single Point. That's the basic building block of all coordinate systems, to be able to refer to a particular spot. We could plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "ZiTBgttVd0V7",
    "outputId": "18c43c8e-404e-4a19-a00d-574b7194727c"
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "gpd.GeoSeries(Point(my_coordinates)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtrY-iNKfB6M"
   },
   "source": [
    "We can plot multiple coordinates, if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "iwxs1fJbe7nj",
    "outputId": "31454214-7131-4045-be75-58a530bbf533"
   },
   "outputs": [],
   "source": [
    "gpd.GeoSeries([Point(1, 7), Point(6, 2), Point(8, 8), Point(5, 9)]).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiKkrzODgGTH"
   },
   "source": [
    "These points could represent anything - the positions of people, or trees, or points along a cycle path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh6fI9_AidTN"
   },
   "source": [
    "## 2. Lines\n",
    "\n",
    "What if it is points along a cycle path, though? Is separate points the best way to show this? Might it not be better to join them up to show the actual path?\n",
    "\n",
    "Yes, we can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "RDMbLCn3gtSH",
    "outputId": "bc8d895c-32f7-4090-cb11-80c99368b720"
   },
   "outputs": [],
   "source": [
    "from shapely import LineString\n",
    "gpd.GeoSeries(LineString([(1, 7), (6, 2), (8, 8), (5, 9)])).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YblUQCx0hwsj"
   },
   "source": [
    "In Python, the module `shapely` is used to define objects of different geometries. While tuples are a built-in basic data type in Python, shapely geometry objects such as Point and Linestring are defined within the shapely module. Once you're using that module, though, these have exactly the same validity as built-in data types like lists and tuples.\n",
    "\n",
    "It's not just Python and shapely, though - the same is true of any geospatial software. Some may do it in different ways, but all will have defined data types for point coordinates, and for lines which are formed from multiple point coordinates.\n",
    "\n",
    "It's a simple matter of associating the points together, so that the program understands those points aren't just individual points, but are actually connected together.\n",
    "\n",
    "One benefit of defining a data type for this is that the order of points in the line is fixed. After all, the coordinates above could form a very different line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "ZAvSsnyphwAt",
    "outputId": "60d4e9b9-a20d-4b8e-c8d2-d9eb1d6190b7"
   },
   "outputs": [],
   "source": [
    "gpd.GeoSeries(LineString([(6, 2), (5, 9), (1, 7), (8, 8)])).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj_q-hcukcSD"
   },
   "source": [
    "That's the same points as the previous - just in a different order. Using a shapely Linestring object for them keeps them in an intended order, and avoids things like accidentally rearranging the points - not something you'd want to do if you're planning where to build a cycle lane, for example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKVBIqKSkvo9"
   },
   "source": [
    "## 3. Polygons\n",
    "\n",
    "What if our coordinates aren't a cycle lane, but are the boundary of a field? In other words, what if the two ends of the line shouldn't just stop, but should be joined together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "ceXuS8SNkulI",
    "outputId": "1a18f6fc-7cca-45bc-bc79-f42106219f5b"
   },
   "outputs": [],
   "source": [
    "from shapely import Polygon\n",
    "gpd.GeoSeries(Polygon([(1, 7), (6, 2), (8, 8), (5, 9)])).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32C9pv8AlGnm"
   },
   "source": [
    "You can see in that code cell that we call these polygons. So, an open series of coordinates, where the ends are not connected to each other, would be a LineString, but where they join together they form a Polygon.\n",
    "\n",
    "These are the three fundamental geometry types in what is referred to as Vector geometry. Each of these shapes - Points, LineStrings, and Polygons - are defined by numerical coordinates for single points, which can remain as single points or form part of an ordered series which can be open or closed.\n",
    "\n",
    "We can define whatever shape we want using these.\n",
    "\n",
    "The position of a person, or a tree, the corner of a house, the centre of a roundabout, the location of a water level monitor in a river, or an air pollution monitor beside a road: Point.\n",
    "\n",
    "The path of a cycle lane or a road, a fence, the course of a river, the path of a bird in flight: LineString.\n",
    "\n",
    "The outline of a house, the area covered by a flood, the boundaries of a field, the border of a country: Polygon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkCWKz0FnwIg"
   },
   "source": [
    "## 4. Multi-part shapes\n",
    "\n",
    "Sometimes, you'll want to keep a collection of points together - not because they're forming a line, but because they're all part of one object. For example, a fixed set of sample locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "-SsC1n7KqPr8",
    "outputId": "3b8588be-3e11-4b76-b44f-0f8a94175011"
   },
   "outputs": [],
   "source": [
    "from shapely import MultiPoint\n",
    "my_multipoint = MultiPoint([(1, 7), (6, 2), (8, 8), (5, 9)])\n",
    "\n",
    "gpd.GeoSeries(my_multipoint).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIIrrYDHrbG7"
   },
   "source": [
    "While this looks the same as our original points plot, it's stored as a single object, which means we can perform operations on it as a single object.\n",
    "\n",
    "Or you might have a collection of lines. For example, a road might include a line for each side, plus lines for each lane, a cycle path, a footpath. You might have a collection of lines to represent a river network which includes tributaries, and so can't be represented by one single line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "Ww30hJAPr_ge",
    "outputId": "231a740a-9836-40a8-e440-65ffeb81f89d"
   },
   "outputs": [],
   "source": [
    "from shapely import MultiLineString\n",
    "my_multilinestring = MultiLineString([[(6, 2), (4, 5), (7, 7), (5, 9)], [(4, 5), (1, 7)], [(7, 7), (8, 8)]])\n",
    "\n",
    "gpd.GeoSeries(my_multilinestring).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2dgYhIJtcQy"
   },
   "source": [
    "Likewise, you might have a collection of polygons which represent a single object - for example a property which comprises multiple buildings, or mutiple land parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "JSun6vqlt_PN",
    "outputId": "ff4aaa54-69e7-463e-e8e2-30856b472b5f"
   },
   "outputs": [],
   "source": [
    "from shapely import MultiPolygon\n",
    "my_multipolygon = MultiPolygon([[((1, 7), (6, 2), (8, 8), (5, 9))], [((12, 14), (16, 17), (15, 14), (11, 10))]])\n",
    "\n",
    "gpd.GeoSeries(my_multipolygon).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1rnt6D5wFMF"
   },
   "source": [
    "## 5. Arrays, series, and dataframes\n",
    "\n",
    "If we have a lot of variables of the same kind, it would be inefficient to store them all separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 9\n",
    "c = 8\n",
    "d = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these variables is going to be stored in a different chunk of memory. If we want to do some data transformation to all of them, we could do it, but it would be rather awkward and slow - especially, as is usually the case with geospatial datasets, you have thousands of numbers, not just four or five. Some of the data I work with involves 100,000 datapoints for a day - and I've processed a over a year's worth of data in one go. Something in the region of 50 million data points. If they were all separate variables, that would take a very long time.\n",
    "\n",
    "Instead, thanks to the library (_=a collection of modules_) _NumPy_, we can store a set of related numbers as an array. \n",
    "\n",
    "<img src=\"https://github.com/numpy/numpy/blob/main/branding/logo/primary/numpylogo.png?raw=true\" style=\"height:60px\" alt=\"NumPy logo\"/>\n",
    "\n",
    "A NumPy array is of a single data type, so the interpreter doesn't have to check the type for each variable, and it holds the values in contiguous blocks of memory, so that operations don't involve the computer searching all over the drive to find the next variable. \n",
    "\n",
    "There's now other libraries which do the same thing, in slightly different ways - one I'm growing quite attached to is _Apache Arrow_. \n",
    "\n",
    "<img src=\"https://arrow.apache.org/img/arrow-logo_horizontal_black-txt_white-bg.png\" style=\"height:60px\" alt=\"Apache Arrow logo\"/>\n",
    "\n",
    "But that's complicated territory, so just remember the name for when you get there yourself.\n",
    "\n",
    "Now, a NumPy or Arrow array is just a list of numbers, it doesn't have anything associated with it. One of the most powerful libraries for data processing is _pandas_.\n",
    "\n",
    "<img src=\"https://pandas.pydata.org/static/img/pandas.svg\" style=\"height:60px\" alt=\"Pandas logo\"/>\n",
    "\n",
    "Pandas uses NumPy or Arrow arrays to construct either a DataSeries or DataFrame. You don't need to even interact with NumPy for this - it's all handled by pandas behind the scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = pd.Series([5, 9, 8, 3])\n",
    "series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of variables a-d, we now have a series which is _indexed_. And we can do a lot of powerful operations on that series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "hghg = dt.now()\n",
    "print(hghg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a small example. And what if we have more than one list of related data? For example, my air pollution research involves measuring particulate pollution of two different sizes, along with pressure, temperature, and humidity. Plus date and time of measurement. That could be 6 separate series - or it could be one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Measurement_time\": [\"2023-11-01 08:00:00\", \"2023-11-01 08:00:30\", \"2023-11-01 08:01:00\", \"2023-11-01 08:01:30\", \"2023-11-01 08:02:00\"], \"PM2.5\": [3,4,6,2,3], \"PM10\": [5,6,5,3,4], \"Pressure\": [1012,1012,1012,1013,1013], \"Temperature\": [12,12,13,13,13], \"Humidity\": [86,86,87,87,87]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're cooking. \n",
    "\n",
    "Oh by the way, let me take this opportunity to demonstrate the point of using ```black``` for code formatting. The above cell is without using black. Here's the same code but using black:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Measurement_time\": [\n",
    "            \"2023-11-01 08:00:00\",\n",
    "            \"2023-11-01 08:00:30\",\n",
    "            \"2023-11-01 08:01:00\",\n",
    "            \"2023-11-01 08:01:30\",\n",
    "            \"2023-11-01 08:02:00\",\n",
    "        ],\n",
    "        \"PM2.5\": [3, 4, 6, 2, 3],\n",
    "        \"PM10\": [5, 6, 5, 3, 4],\n",
    "        \"Pressure\": [1012, 1012, 1012, 1013, 1013],\n",
    "        \"Temperature\": [12, 12, 13, 13, 13],\n",
    "        \"Humidity\": [86, 86, 87, 87, 87],\n",
    "    }\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how much more readable that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. GeoSeries and GeoDataFrames: GeoPandas\n",
    "\n",
    "We can do even better, though. Build on top of pandas is the library GeoPandas, which adds geospatial properties and functionality to pandas. \n",
    "\n",
    "<img src=\"https://geopandas.org/en/stable/_images/geopandas_logo.png\" style=\"height:60px\" alt=\"GeoPandas logo\"/>\n",
    "\n",
    "A GeoPandas GeoSeries is a subclass of a pandas Series, and a GeoDataFrame is a subclass of a pandas DataFrame. So, anything you can do in pandas, you can do in GeoPandas - now with added geospatial information.\n",
    "\n",
    "A GeoSeries is a series of Shapely Point, Line, or Polygon coordinates: a geometry series.\n",
    "\n",
    "A GeoDataFrame is a DataFrame with one column being a geometry GeoSeries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, when I'm using these abbreviations on import - these aren't mine, these are conventions. I'm not able to say for sure how they got to be conventions, but in any case, the vast majority of code you'll find online has\n",
    "\n",
    "```import numpy as np```  \n",
    "```import pandas as pd```  \n",
    "```import geopandas as gpd```  \n",
    "\n",
    "Some are probably just from common use, but there are some which came from the creator. For example, we probably won't get to consider seaborn, which is another plotting library, but you'll probably come across it eventually. You might think it should be import seaborn as sb or sn, but it's actually\n",
    "\n",
    "```import seaborn as sns```\n",
    "\n",
    "because that's the initials of the character Samuel Normal Seaborn from The West Wing. \n",
    "\n",
    "Python people are just like this sometimes, I don't know what to tell you. I'm right there as well, and I'm not apologising for it. Also, I might mention that Python is not named after the snake, even if that's the interpretation implied by the existence of Anaconda - it's named after Monty Python, and you will very often see python tutorials using Monty Python references in their example code, e.g. ```spam``` for variable names. If you don't like it, well, the R tutorial is at 11.30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"Meas_time\": [\n",
    "            \"2023-11-01 08:00:00\",\n",
    "            \"2023-11-01 08:00:30\",\n",
    "            \"2023-11-01 08:01:00\",\n",
    "            \"2023-11-01 08:01:30\",\n",
    "            \"2023-11-01 08:02:00\",\n",
    "        ],\n",
    "        \"PM2.5\": [3, 4, 6, 2, 3],\n",
    "        \"PM10\": [5, 6, 5, 3, 4],\n",
    "        \"Pressure\": [1012, 1012, 1012, 1013, 1013],\n",
    "        \"Temperature\": [12, 12, 13, 13, 13],\n",
    "        \"Humidity\": [86, 86, 87, 87, 87],\n",
    "        \"geometry\": [Point(555173, 654321), Point(555173, 654321), Point(555173, 654321), Point(555173, 654321), Point(555173, 654321)],\n",
    "    }\n",
    ")\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're _really_ getting somewhere.\n",
    "\n",
    "Where are we getting though? Because we don't know what type of coordinates they are, do we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeoDataFrame is a class in GeoPandas. We saw in our class definition above that classes can have properties which are defined as ```self.property```. The GeoDataFrame class includes a ```self.crs``` property describing the coordinate reference system of the data. This defaults to None, which isn't particularly useful, but isn't as bad as having a wrong crs. The CRS should be declared when creating the GeoDataFrame, for example the previous should be been created as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"Meas_time\": [\n",
    "            \"2023-11-01 08:00:00\",\n",
    "            \"2023-11-01 08:00:30\",\n",
    "            \"2023-11-01 08:01:00\",\n",
    "            \"2023-11-01 08:01:30\",\n",
    "            \"2023-11-01 08:02:00\",\n",
    "        ],\n",
    "        \"PM2.5\": [3, 4, 6, 2, 3],\n",
    "        \"PM10\": [5, 6, 5, 3, 4],\n",
    "        \"Pressure\": [1012, 1012, 1012, 1013, 1013],\n",
    "        \"Temperature\": [12, 12, 13, 13, 13],\n",
    "        \"Humidity\": [86, 86, 87, 87, 87],\n",
    "        \"geometry\": [Point(555173, 654321), Point(555173, 654321), Point(555173, 654321), Point(555173, 654321), Point(555173, 654321)],\n",
    "    },\n",
    "    crs=2157\n",
    ")\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Coordinate Reference Systems\n",
    "\n",
    "pyproj is a library which is designed to handle coordinate reference systems. \n",
    "\n",
    "<img src=\"https://pyproj4.github.io/pyproj/stable/_static/logo.png\" style=\"height:60px\" alt=\"pyproj logo\"/>\n",
    "\n",
    "Like NumPy, it is written in C, so although it's called from python, all the computation is done at a compiled level. \n",
    "\n",
    "pyproj can interpret references to a CRS in numerous different formats. From the pyproj documentation (https://pyproj4.github.io/pyproj/stable/api/crs/crs.html#pyproj.crs.CRS.from_user_input):\n",
    "\n",
    " -  PROJ string  \n",
    " - Dictionary of PROJ parameters \n",
    " - PROJ keyword arguments for parameters  \n",
    " - JSON string with PROJ parameters  \n",
    " - CRS WKT string  \n",
    " - An authority string, e.g. ```'epsg:4326'```  \n",
    " - An EPSG integer e.g. ```'4326'```  \n",
    " - A tuple of (“auth_name”: “auth_e.g.”) e.g. ```('epsg',   '4326')```  \n",
    " - An object with a ```to_wkt``` method  \n",
    " - A ```pyproj.crs.CRS``` class \n",
    "\n",
    "pyproj allows for the definition of custom coordinate reference systems, and as per the list above, can essentially handle any pre-defined crs. It also facilitates transformations between coordinate reference systems. Using the above example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.to_crs(4326)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.to_crs('+proj=laea +lat_0=52 +lon_0=10 +x_0=4321000 +y_0=3210000 +ellps=GRS80 +units=m +no_defs +type=crs')\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Reading and writing files\n",
    "\n",
    "So, now you know you can handle geospatial vector data in a GeoPandas GeoDataFrame, using NumPy (or Arrow) behind the scenes, with Shapely handling the vector coordinates and pyproj managing the CRS. That's a pretty good place to start, but the one thing we haven't covered yet is getting the data in and out. In the examples above, we just used data which had been typed in - but again, you could be dealing with datasets including millions of rows or columns. Nobody's typing that in. So, we need to be able to read and write files.\n",
    "\n",
    "Of course, this is also built in. GeoPandas uses <a href='https://fiona.readthedocs.io/en/stable/manual.html'>_Fiona_</a> to handle <a href='https://geopandas.org/en/stable/docs/user_guide/io.html'>file reading</a>. The Fiona library can handle pretty much any geospatial file format you can think of. Let's import some air pollution sensor data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_file('../sample_data/LimerickAir10_2021-01-22_FrRRd.csv')\n",
    "pollution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```geopandas.GeoDataFrame.head()``` shows the top rows of the dataframe, just as a quick view. \n",
    "\n",
    "Note the _%time_ magic marker - that just measures how long a process takes. It's not core python, but it is a core part of the ipykernel used by Jupyter. Handy! There's <a href='https://ipython.readthedocs.io/en/stable/interactive/magics.html'>other magic markers</a> too.\n",
    "\n",
    "So this has geospatial data, but it's a CSV file, and the coordinates are just as X and Y columns, not shapely Points. Easy fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution = pollution.set_geometry(gpd.points_from_xy(pollution['X'], pollution['Y']))\n",
    "pollution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a CRS defined, so let's add that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pollution.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution = pollution.set_crs(2157)\n",
    "print(pollution.crs)\n",
    "pollution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the X and Y columns anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution = pollution.drop(['X', 'Y'], axis=1)\n",
    "pollution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, GeoPandas can also save files. It's not just limited to the traditional ```.shp``` or ```.gpkg``` files either. A couple of file formats you should be aware of: parquet, and feather. Both are efficient binary representations of columnar data, with feather being a hard drive representation of the in-memory Apache Arrow format which I mentioned above as an alternative to NumPy arrays. Let's do some timed saves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution.to_file('pollution.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am begging you, please, <a href='http://switchfromshapefile.org/'>_stop using shapefiles_</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution.to_file('pollution.gpkg', driver='GPKG', layer='name') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than shapefile, but let's be honest - that's a _low_ bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution.to_csv('pollution.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/B2l0NnxK9KiVa0CXBh/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution.to_parquet('pollution.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollution['geometry'].isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution.to_feather('pollution.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/29u2oG8ymBrri/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it the same for reading the files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_file(\"pollution.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![display image](https://media.giphy.com/media/27tE5WpzjK0QEEm0WC/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_file(\"pollution.gpkg\", layer='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be fully fair to these two formats, GeoPandas does have an alternative to Fiona for reading files - _pyogrio_ will be the default file i/o engine in version 1.0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_file(\"pollution.shp\", engine=\"pyogrio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_file(\"pollution.gpkg\", layer='name', engine=\"pyogrio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a big improvement. But..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_parquet(\"pollution.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pollution = gpd.read_feather(\"pollution.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![display image](https://media.giphy.com/media/cnqbWwFsnRcGs/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get my point - consider the parquet and feather formats. Although, it's not too bad a result using pandas.read_csv() either:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Vector Geoprocessing\n",
    "\n",
    "Geopandas can also handle some <a href='https://geopandas.org/en/stable/docs/user_guide/geometric_manipulations.html'>geoprocessing</a> of vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas = gpd.read_file('https://opendata.arcgis.com/api/v3/datasets/70a33cbb8bd7406da0d571be28624721_0/downloads/data?format=shp&spatialRefId=2157&where=1%3D1')\n",
    "small_areas.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas = small_areas.rename(columns = {'SA_GUID_20':'GUID'})\n",
    "small_areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas_data = pd.read_csv('https://www.cso.ie/en/media/csoie/census/census2022/SAPS_2022_Small_Area_270923.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas = small_areas.merge(small_areas_data, on='GUID')\n",
    "small_areas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas.plot(column='T1_1AGETT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas_urb = small_areas.dissolve(by='SA_URBAN_1', aggfunc='sum')\n",
    "small_areas_urb.plot(column = 'T1_1AGETT', scheme='quantiles', cmap='YlOrRd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_areas_urb.buffer(5000).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, there's much more where that came from - it's just a case of finding the methods that you need in the documentation. And it's not the only library we can use for vector geoprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = \"Limerick, Ireland\"\n",
    "tags = {\"building\": True}\n",
    "lk_buildings = ox.features_from_place(place, tags)\n",
    "lk_buildings = lk_buildings[lk_buildings.geom_type == 'Polygon']\n",
    "lk_buildings = lk_buildings.to_crs(2157)\n",
    "lk_buildings.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all the buildings in Limerick, pulled from OpenStreetMap by the library <a href='https://geoffboeing.com/2016/11/osmnx-python-street-networks/'>osmnx</a>. There's <a href='https://github.com/gboeing/osmnx-examples/tree/main/notebooks'>some great examples</a> of how to use this by the author Geoff Boeing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_buildings_sa = lk_buildings.overlay(small_areas, how='intersection')\n",
    "lk_buildings_sa.plot(column=\"GUID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted by colour according to which CSO Small Area they're in.\n",
    "\n",
    "Or, how about if you want to map moving objects, say a bus journey?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import movingpandas as mpd\n",
    "\n",
    "gdf = gpd.read_file('../sample_data/bus_journey.gpx', layer=\"track_points\").set_index(\"time\")\n",
    "gdf.drop(\n",
    "    columns=[\n",
    "        \"magvar\",\n",
    "        \"geoidheight\",\n",
    "        \"name\",\n",
    "        \"cmt\",\n",
    "        \"desc\",\n",
    "        \"src\",\n",
    "        \"link1_href\",\n",
    "        \"link1_text\",\n",
    "        \"link1_type\",\n",
    "        \"link2_href\",\n",
    "        \"link2_text\",\n",
    "        \"link2_type\",\n",
    "        \"sym\",\n",
    "        \"type\",\n",
    "        \"fix\",\n",
    "        \"sat\",\n",
    "        \"hdop\",\n",
    "        \"vdop\",\n",
    "        \"pdop\",\n",
    "        \"ageofdgpsdata\",\n",
    "        \"dgpsid\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "track = mpd.Trajectory(gdf, 1)\n",
    "track.add_speed(name=\"speed (km/h)\", units=(\"km\", \"h\"))\n",
    "track.add_distance(units=\"m\")\n",
    "\n",
    "track.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track.plot(\n",
    "    column='speed (km/h)',\n",
    "    line_width=7.0,\n",
    "    cmap=\"RdYlGn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "lk_buildings_sa.plot(column=\"T1_1AGETT\", scheme='quantiles', cmap='YlOrRd', ax=ax)\n",
    "track.to_crs(2157).plot(\n",
    "    column='speed (km/h)',\n",
    "    line_width=7.0,\n",
    "    cmap=\"RdYlGn\",\n",
    "    ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a taste - the headline really is that if you can do it in GIS, you can do it more easily in Python. And if you can't do it in GIS, you can often still do it in Python anyway. Oh, and I haven't even gone near machine learning - we certainly don't have time for that, but when you're ready, have a look at Scikit-learn, PyTorch, and TensorFlow."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4LtCeYTgCUw2pjPTWnXeO",
   "include_colab_link": true,
   "mount_file_id": "16H5yqJLpYgLlFHoHiOcksWzRLYmGVa_4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "IEOS2023",
   "language": "python",
   "name": "ieos2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
